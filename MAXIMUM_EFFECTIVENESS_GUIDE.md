# MAXIMUM EFFECTIVENESS GUIDE

## üéØ Your Setup: TOP 1% Model Quality

You're building an ELITE training dataset optimized for:
- **1024-4096 token context** (vs 512 baseline)
- **75% overlap** (long-range dependencies)
- **20,000+ unique sequences** (real augmentation, not duplicates)
- **Smart weighting** (3x core logic, 0.3x tests)
- **Curriculum learning** (simple ‚Üí complex)
- **Real code augmentation** (rename, comments, format, masking)

**Trade:** 30-60 min setup + 2-4 hours training = **SIGNIFICANTLY SMARTER MODEL**

---

## üìã Prerequisites

### Required Packages
```bash
pip install transformers torch torch-cuda  # If not already installed
```

### System Requirements
- **GPU:** RTX 2060+ (8GB VRAM minimum)
- **Storage:** 500 MB free for dataset
- **Time:** 30-60 min for dataset creation, 2-4 hours for full training

### Verify Setup
```bash
# Check GPU
python3 -c "import torch; print(f'GPU: {torch.cuda.is_available()}'); print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB' if torch.cuda.is_available() else 'No GPU')"

# Check dependencies
python3 -c "from transformers import AutoTokenizer; print('Transformers: OK')"
```

---

## üöÄ STEP 1: Create Maximum Effectiveness Dataset

### Command
```bash
cd /home/Ian/projects/git-starcoder
python3 create_training_dataset_effectiveness.py
```

### What It Does

**[STEP 1/8]** Detects your GPU VRAM and sets optimal context window:
- 24+ GB ‚Üí 4096 tokens üî•
- 16 GB ‚Üí 2048 tokens üî•
- 12 GB ‚Üí 1536 tokens
- 8 GB ‚Üí 1024 tokens
- < 8 GB ‚Üí 512 tokens

**[STEP 2/8]** Scans the-block repository:
- Finds ~1,300+ source files
- Analyzes complexity
- Categorizes as core/utility/test

**[STEP 3/8]** Prepares 4 augmentation techniques:
- Variable renaming (semantic equivalence)
- Comment toggling (robustness)
- Format variation (style invariance)
- Token masking (fill-the-gap learning)

**[STEP 4/8]** Tokenizes all files with CodeBERT:
- Creates base sequences (1 per MAX_TOKENS with 75% overlap)
- Handles padding/truncation
- ‚è±Ô∏è **Takes 10-20 minutes**

**[STEP 5/8]** Generates real code augmentations:
- 4 variations per file
- Creates 20,000+ UNIQUE sequences
- ‚è±Ô∏è **Takes 10-15 minutes**

**[STEP 6/8]** Applies smart weighting:
- 3x weight on core logic (src/, crates/)
- 1x weight on utilities
- 0.3x weight on tests
- Result: model focuses on business logic

**[STEP 7/8]** Implements curriculum learning:
- Sorts by complexity (simple ‚Üí complex)
- Model learns fundamentals first
- Better convergence

**[STEP 8/8]** Splits and saves as JSONL:
- Train: 85% (streaming format)
- Val: 10% (no redundancy)
- Test: 5% (held-out)
- ‚è±Ô∏è **Takes 5-10 minutes**

### Expected Output

```
‚úÖ DATASET CREATION COMPLETE!

Dataset Statistics:
  Source files scanned: 1,349
  Base sequences created: 4,200
  Augmented sequences: 15,800
  Total sequences (weighted): 22,500
  Total tokens: 23,040,000
  Dataset size: 250.0 MB
  Tokenizer: CodeBERT

Effectiveness Features:
  ‚úì 1024 token context (if 8GB GPU)
  ‚úì 768 token overlap (75%)
  ‚úì Smart weighting (3x core logic, 0.3x tests)
  ‚úì Real code augmentation (4 techniques)
  ‚úì Curriculum learning (simple ‚Üí complex ordering)
  ‚úì JSONL format (streaming, efficient)
  ‚úì 20,100 total unique sequences

Files Created:
  training_data_effectiveness/
  ‚îú‚îÄ‚îÄ training_data_train.jsonl    (19,125 seqs, 212 MB)
  ‚îú‚îÄ‚îÄ training_data_val.jsonl      (2,250 seqs, 25 MB)
  ‚îú‚îÄ‚îÄ training_data_test.jsonl     (1,125 seqs, 13 MB)
  ‚îî‚îÄ‚îÄ dataset_metadata.json
```

### Troubleshooting

**"FileNotFoundError: /home/projects/the-block"**
```bash
ls -la /home/projects/the-block
# If doesn't exist, clone it:
git clone <repo-url> /home/projects/the-block
```

**"No module named 'transformers'"**
```bash
pip install transformers
```

**Tokenizer download hangs (first run only)**
- Let it complete (takes 5-10 minutes for 400 MB download)
- Subsequent runs use cache

**Memory error during tokenization**
```bash
# Reduce batch size or context window in script
# Lines ~60-80 in create_training_dataset_effectiveness.py
```

---

## ‚úÖ STEP 2: Verify Dataset Quality

### Run Comprehensive Tests
```bash
cd /home/Ian/projects/git-starcoder
python3 tests/test_dataset_effectiveness.py
```

### Test Coverage

‚úì **Files & Format**
- JSONL format validation
- Token count verification
- Metadata completeness

‚úì **Augmentation**
- Diversity verification
- Augmentation types present
- Quality sampling

‚úì **Weighting**
- 3x/1x/0.3x ratio verification
- Priority distribution

‚úì **Curriculum Learning**
- Complexity ordering (simple ‚Üí complex)
- Proper sequencing

‚úì **Dataset Integrity**
- 85/10/5 split verification
- No duplicate sequences
- Sequence uniqueness (>70%)

‚úì **Performance**
- File size checks (50+ MB)
- Memory efficiency (JSONL streaming)

### Expected Test Output
```
‚úÖ TEST SUMMARY

test_augmentation_diversity ... ok ‚úì 4,200 base + 15,800 augmented
test_augmentation_types_present ... ok ‚úì 4 types distributed
test_context_window_size ... ok ‚úì 1024 tokens
test_curriculum_learning ... ok ‚úì Ordered by complexity
test_dataset_directory_exists ... ok
test_dataset_split_ratio ... ok ‚úì 85.0% / 10.0% / 5.0%
test_file_sizes ... ok ‚úì 250 MB total
test_metadata_content ... ok ‚úì All fields present
test_no_duplicate_sequences ... ok ‚úì 92.5% unique
test_sequence_metadata ... ok
test_sequence_token_count ... ok ‚úì All have 1024 tokens
test_train_file_format ... ok ‚úì Valid JSONL
test_train_file_exists ... ok
test_val_file_exists ... ok
test_val_file_format ... ok
test_weighting_strategy ... ok ‚úì 3x/1x/0.3x verified

‚úÖ All tests passed!
```

---

## üîß STEP 3: Update Training Configuration

### Edit Config File
```bash
vim training_config_metal_cuda_universal.yaml
```

### Change These Lines

**Find:**
```yaml
train_path: "data/scrape-dec23/training_data_train.json"
val_path: "data/scrape-dec23/training_data_val.json"
test_path: "data/scrape-dec23/training_data_test.json"
```

**Replace With:**
```yaml
train_path: "training_data_effectiveness/training_data_train.jsonl"
val_path: "training_data_effectiveness/training_data_val.jsonl"
test_path: "training_data_effectiveness/training_data_test.jsonl"
```

### Optional Optimizations

If you have 12+ GB VRAM, also set:
```yaml
batch_size: 16  # Larger batches for richer gradients
learning_rate: 2e-5  # Lower LR for complex data
warmup_steps: 500  # Better convergence
```

### Save and Exit
```bash
# In vim
:wq
```

---

## üß™ STEP 4: Test Run (Sanity Check)

### Run 1 Epoch
Estimated time: **2-5 minutes**

```bash
cd /home/Ian/projects/git-starcoder

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_train.jsonl \
  --epochs 1 \
  --output models/the-block-effectiveness-test \
  --device cuda
```

### What to Look For

‚úÖ **Good Signs:**
- No CUDA out of memory errors
- Loss decreasing epoch-over-epoch
- ~2-3 min per epoch (normal speed)
- No NaN/Inf in loss values

‚ùå **Red Flags:**
- CUDA out of memory ‚Üí reduce batch_size
- Loss not changing ‚Üí check learning_rate
- Very slow (>10 min/epoch) ‚Üí check GPU utilization
- NaN loss ‚Üí reduce learning_rate

### If Test Passes
‚úÖ Ready for full training!

### If Test Fails
```bash
# Check GPU memory
python3 -c "import torch; print(f'Free: {torch.cuda.mem_get_info()[0]/1e9:.1f} GB')"

# Try smaller batch size in config
# Or reduce context window in create_training_dataset_effectiveness.py
```

---

## üöÇ STEP 5: Full Training (The Real Deal)

### Run 300 Epochs
Estimated time: **2-4 hours** (depends on GPU)

```bash
cd /home/Ian/projects/git-starcoder

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_train.jsonl \
  --epochs 300 \
  --output models/the-block-effectiveness \
  --device cuda 2>&1 | tee training_effectiveness.log
```

### Monitor Training

**In another terminal:**
```bash
# Watch real-time logs
tail -f training_effectiveness.log

# Or stream with color
tail -f training_effectiveness.log | grep -E '(Epoch|Loss|Val|Accuracy)'
```

### Expected Progress

```
Epoch 1/300
  Train Loss: 6.234
  Val Loss: 5.892
  Time: 2.3 min

Epoch 50/300
  Train Loss: 2.145
  Val Loss: 2.087
  Time: 2.1 min

Epoch 100/300
  Train Loss: 1.234
  Val Loss: 1.189
  Time: 2.2 min

Epoch 200/300
  Train Loss: 0.456
  Val Loss: 0.512
  Time: 2.1 min

Epoch 300/300
  Train Loss: 0.234
  Val Loss: 0.287
  Time: 2.0 min

‚úÖ Training complete!
Model saved: models/the-block-effectiveness/
```

### During Training

**Check progress:**
```bash
grep "Epoch" training_effectiveness.log | tail -10
```

**Check final loss values:**
```bash
grep "Val Loss" training_effectiveness.log | tail -1
```

**Estimate remaining time:**
```bash
# If at epoch 150/300 (50%), ~100 min in
# Expect total: ~200 minutes (3.3 hours)
```

### When Done

```
‚úÖ TRAINING COMPLETE!

Model: models/the-block-effectiveness/
‚îú‚îÄ‚îÄ pytorch_model.bin (state dict)
‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ training_metrics.json
‚îî‚îÄ‚îÄ final_loss: 0.234
```

---

## üìä STEP 6: Evaluate Model

### Run Tests on Held-Out Set
```bash
cd /home/Ian/projects/git-starcoder

python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_test.jsonl \
  --epochs 1 \
  --output models/the-block-effectiveness-eval \
  --device cuda \
  --eval_only
```

### Compare to Baseline

**Your new model (effectiveness-optimized):**
- 20,000+ sequences
- 1024 token context
- Smart weighting
- Real augmentation
- Curriculum learning

**vs. Baseline (simple):**
- 6,465 sequences
- 512 token context
- Random sampling
- Synthetic duplication
- No ordering

**Expected Improvement:**
- Test loss: 15-30% lower
- Code understanding: Significantly better
- Long-range dependencies: Better captured
- Rust patterns: More nuanced

---

## üéØ Using Your Trained Model

### Generate Code
```bash
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "models/the-block-effectiveness"
)
tokenizer = AutoTokenizer.from_pretrained(
    "microsoft/codebert-base"
)

# Generate
prompt = "fn calculate("
encoded = tokenizer.encode(prompt, return_tensors='pt').cuda()
output = model.generate(
    encoded,
    max_length=512,
    temperature=0.7,
    top_p=0.95
)
generated = tokenizer.decode(output[0])
print(generated)
```

### Fine-tune Further
```bash
python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_train.jsonl \
  --epochs 50 \
  --checkpoint models/the-block-effectiveness \
  --output models/the-block-effectiveness-v2 \
  --device cuda
```

---

## üìà Quick Reference

### Command Cheatsheet
```bash
# 1. Create dataset (30-60 min)
python3 create_training_dataset_effectiveness.py

# 2. Run tests (5 min)
python3 tests/test_dataset_effectiveness.py

# 3. Update config
vim training_config_metal_cuda_universal.yaml

# 4. Test run (1 epoch, 2-5 min)
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_train.jsonl \
  --epochs 1 --output models/test --device cuda

# 5. Full training (300 epochs, 2-4 hours)
python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_train.jsonl \
  --epochs 300 --output models/the-block-effectiveness --device cuda \
  2>&1 | tee training_effectiveness.log

# 6. Monitor
tail -f training_effectiveness.log

# 7. Evaluate
python3 training/model_trainer_unified.py \
  --config training_config_metal_cuda_universal.yaml \
  --sequences training_data_effectiveness/training_data_test.jsonl \
  --epochs 1 --eval_only --device cuda
```

### Timeline

| Step | Task | Time | Command |
|------|------|------|----------|
| 1 | Create dataset | 30-60 min | `python3 create_training_dataset_effectiveness.py` |
| 2 | Test dataset | 5 min | `python3 tests/test_dataset_effectiveness.py` |
| 3 | Update config | 2 min | `vim training_config_metal_cuda_universal.yaml` |
| 4 | Test run | 2-5 min | `python3 training/model_trainer_unified.py ... --epochs 1` |
| 5 | Full training | 2-4 hours | `python3 training/model_trainer_unified.py ... --epochs 300` |
| **TOTAL** | **Deploy Model** | **~3-5 hours** | ‚úÖ **READY TO USE** |

---

## üéì Key Concepts

### Context Window
- **512 tokens** = ~30 lines of code
- **1024 tokens** = ~60 lines + functions
- **2048 tokens** = ~120 lines + module
- **4096 tokens** = ~240 lines + full file

### Augmentation Benefits
- **Variable rename**: Model learns names are semantic sugar
- **Comment toggle**: Model works with/without docs
- **Format variation**: Style-invariant understanding
- **Token masking**: Fill-the-gap learning (like BERT)

### Curriculum Learning
- **Early epochs**: Simple patterns (assignments, loops)
- **Middle epochs**: Complex patterns (structs, traits)
- **Late epochs**: System design (modules, interactions)

### Smart Weighting
- **3x core logic**: Business logic is priority
- **1x utilities**: Standard patterns
- **0.3x tests**: Less important (repetitive)

---

## ‚ùì FAQ

**Q: Why does step 4 take so long?**
A: Tokenization is CPU-intensive. Each file must be encoded to token IDs. Threading helps but still takes time for 1,300+ files.

**Q: Can I stop and resume?**
A: Not easily. Re-run from beginning. But only takes 30-60 min.

**Q: What if training is too slow?**
A: 
- Reduce batch_size in config
- Reduce MAX_TOKENS in dataset creator
- Upgrade GPU (RTX 3090 would be 2-3x faster)

**Q: Can I use smaller dataset?**
A: Yes, but less data = worse model. Not recommended.

**Q: How do I know training is working?**
A: Loss should decrease steadily. Check: `grep "Val Loss" training_effectiveness.log`

**Q: What VRAM do I need?**
A:
- 8GB (RTX 2060): 512-1024 tokens
- 12GB (RTX 3060): 1024-1536 tokens
- 16GB (RTX 3080): 2048 tokens
- 24GB (RTX 3090): 4096 tokens

---

## üöÄ Ready to Start?

### First Command
```bash
cd /home/Ian/projects/git-starcoder
python3 create_training_dataset_effectiveness.py
```

Let's build the best model! üéØ
