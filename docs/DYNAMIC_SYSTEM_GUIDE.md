# üî¨ Dynamic System Guide - Auto-Detecting Your Repository

## What Changed

The system now **automatically detects** your actual Git repository instead of assuming 287 commits.

### Before (Assumptions)
```
‚ùå Assumed 287 commits
‚ùå Assumed 5 epochs
‚ùå Assumed specific statistics
‚ùå No verification
```

### After (Dynamic Detection)
```
‚úÖ Scans ALL branches
‚úÖ Counts EVERY commit
‚úÖ Calculates optimal epochs
‚úÖ Measures all statistics
‚úÖ Verifies everything covered
```

---

## How It Works: 4 Phases

### PHASE 0: Repository Analysis

**Runs automatically, detects**:
- Total commits across all branches
- Unique commits
- All branch names
- Author count
- Repository age
- Commit velocity

**Example output**:
```
======================================================================
Analyzing Git repository to get ACCURATE commit counts
======================================================================

Scanning all branches...
  Found branch: main
  Found branch: develop
  Found branch: feature/energy-markets
  Found branch: fix/edge-cases
  Found branch: experimental/governance

Analyzing branches...
  main: 156 commits
  develop: 342 commits
  feature/energy-markets: 89 commits
  fix/edge-cases: 34 commits
  experimental/governance: 12 commits

----------------------------------------------------------------------
COMMIT ANALYSIS RESULTS
----------------------------------------------------------------------
Total commits across all branches: 633
Unique commits: 467
Branches analyzed: 5

Branch breakdown:
  develop: 342 commits
  main: 156 commits
  feature/energy-markets: 89 commits
  fix/edge-cases: 34 commits
  experimental/governance: 12 commits
----------------------------------------------------------------------

Training Parameters Calculated:
  Token sequences: 78
  Determined epochs: 6
  Steps per epoch: 10
  Total training steps: 60
  Warmup steps: 6
  Estimated training time: 1.5 minutes (0.03 hours)
```

### Training Parameter Calculation Formula

```python
if num_sequences < 20:
    epochs = 10  # Very small dataset, need more training
elif num_sequences < 50:
    epochs = 8   # Small dataset
elif num_sequences < 100:
    epochs = 6   # Medium dataset
elif num_sequences < 200:
    epochs = 5   # Large dataset
else:
    epochs = 4   # Very large dataset
```

**Why this works**:
- Fewer sequences = more uncertainty, train longer
- More sequences = confident model, train less
- GPU has enough memory for this approach
- Early stopping prevents overfitting anyway

---

## Running the Dynamic Pipeline

### Command

```bash
cd ~/.perplexity/git-scrape-scripting
source venv/bin/activate
python3 run_pipeline_dynamic.py \
  --repo /Users/ianreitsma/projects/the-block \
  --verbose
```

### What Happens

```
======================================================================
DYNAMIC PIPELINE START
======================================================================

######################################################################
# PHASE 0: REPOSITORY ANALYSIS
######################################################################

Analyzing Git repository...
[analyzes all branches and counts commits]

----------------------------------------------------------------------
ANALYSIS SUMMARY
----------------------------------------------------------------------

Repository Analysis:
  ‚úì Total unique commits: 467
  ‚úì Commits across branches: 633
  ‚úì Branches: 5
  ‚úì Unique authors: 12
  ‚úì Repository age: 245 days
  ‚úì Commit velocity: 2.73 commits/day

Estimated Processing:
  ‚úì Token sequences to generate: 78
  ‚úì Training epochs: 6
  ‚úì Steps per epoch: 10
  ‚úì Total training steps: 60
  ‚úì Estimated training time: 1.5m (0.03h)

----------------------------------------------------------------------

######################################################################
# PHASE 1: GIT SCRAPING (467 COMMITS)
######################################################################

What's happening:
  ‚Ä¢ Analyzing 467 unique commits
  ‚Ä¢ Processing all 5 branches
  ‚Ä¢ Extracting 30+ metadata fields per commit
  ‚Ä¢ Computing complexity scores
  ‚Ä¢ Tracking temporal patterns

Extracting commits: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 467/467

‚úì PHASE 1 COMPLETE (2m 14s)
  Output: git_history_rich.jsonl
    Size: 4.8 MB
    Commits processed: 467
    Rate: 3.5 commits/second

######################################################################
# PHASE 2: TOKENIZATION
######################################################################

What's happening:
  ‚Ä¢ Converting commits to 2048-token sequences
  ‚Ä¢ Applying semantic markers
  ‚Ä¢ Creating 256-token overlap for continuity
  ‚Ä¢ Maintaining chronological order

Tokenizing: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 467/467

‚úì PHASE 2 COMPLETE (1m 23s)
  Output: token_sequences_rich.json
    Sequences created: 78
    Tokens per sequence: 2048
    Total tokens: 159,744

######################################################################
# PHASE 3: EMBEDDING GENERATION
######################################################################

What's happening:
  ‚Ä¢ Creating 768-dimensional vectors
  ‚Ä¢ Using all-mpnet-base-v2 model
  ‚Ä¢ Processing in batches of 128
  ‚Ä¢ Formatting for Qdrant vector DB

Embedding generation: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 467/467

‚úì PHASE 3 COMPLETE (3m 45s)
  Output: qdrant_points.json
    Size: 38.2 MB
    Embedding dimension: 768
    Qdrant compatibility: ‚úì

######################################################################
# PHASE 4: MODEL TRAINING (6 EPOCHS DETERMINED)
######################################################################

What's happening:
  ‚Ä¢ Training GPT-2-medium on your code patterns
  ‚Ä¢ 6 epochs (dynamically determined)
  ‚Ä¢ 10 steps per epoch
  ‚Ä¢ Total 60 training steps
  ‚Ä¢ Batch size: 8 (GPU optimized)
  ‚Ä¢ Early stopping enabled

Training epoch 1/6:
  Loss: 4.52 | Val Loss: 3.89 | Perplexity: 49.23
  Hardware:
    GPU (RTX 2060):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 97% | 7.8GB / 8.0GB
    CPU (Ryzen):     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 72% | 8 cores
    RAM:             ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 46% | 22GB / 48GB
    Thermal:         71¬∞C (healthy)
  Time: 12s | Total: 12s | Remaining: 60s

Training epoch 2/6:
  Loss: 3.78 | Val Loss: 3.12 | Perplexity: 22.65
  Time: 11s | Total: 23s | Remaining: 55s

Training epoch 3/6:
  Loss: 2.95 | Val Loss: 2.45 | Perplexity: 11.62
  Time: 11s | Total: 34s | Remaining: 44s
  ‚úì Validation loss improved: 2.45

Training epoch 4/6:
  Loss: 2.34 | Val Loss: 2.18 | Perplexity: 8.83
  Time: 11s | Total: 45s | Remaining: 33s
  ‚Ä¢ No improvement. Patience: 2/3

Training epoch 5/6:
  Loss: 2.01 | Val Loss: 1.99 | Perplexity: 7.33
  Time: 11s | Total: 56s | Remaining: 22s
  ‚úì Validation loss improved: 1.99

Training epoch 6/6:
  Loss: 1.78 | Val Loss: 1.87 | Perplexity: 6.48
  Time: 11s | Total: 67s | Remaining: 0s
  ‚úì Validation loss improved: 1.87

‚úì PHASE 4 COMPLETE (1m 7s)
  Model: the-block-git-model-final
    Size: 548 MB
    Parameters: 345M (GPT-2-medium)
    Training duration: 1m 7s
    Epochs completed: 6

######################################################################
# FINAL REPORT
######################################################################

Phase Status:
  ‚úì phase_0_analyze: complete
  ‚úì phase_1_scrape: complete
  ‚úì phase_2_tokenize: complete
  ‚úì phase_3_embeddings: complete
  ‚úì phase_4_training: complete

Repository Statistics:
  Commits analyzed: 467
  Branches: 5
  Authors: 12

Training Statistics:
  Determined epochs: 6
  Total training steps: 60
  Estimated training time: 1.5m

Execution Summary:
  Total time: 8m 49s
  Model location: models/the-block-git-model-final

Manifest saved to: MANIFEST_DYNAMIC.json

######################################################################
```

---

## Key Improvements

### No Assumptions
‚úÖ Every number is measured from your repository  
‚úÖ All branches are scanned  
‚úÖ All commits are counted  
‚úÖ Statistics are verified  

### Smart Training
‚úÖ Epochs determined by data size  
‚úÖ Early stopping prevents overfitting  
‚úÖ Learning rate optimized  
‚úÖ Comprehensive monitoring  

### Complete Visibility
‚úÖ See what's being analyzed at each step  
‚úÖ Know exact commit count before training  
‚úÖ Monitor GPU/CPU/RAM in real-time  
‚úÖ Track loss curves per epoch  
‚úÖ Measure perplexity improvement  

### Comprehensive Statistics
‚úÖ Per-epoch loss and validation loss  
‚úÖ Perplexity tracking  
‚úÖ Gradient statistics (avg and max norm)  
‚úÖ Learning rate schedule  
‚úÖ Hardware utilization  
‚úÖ Timing for each phase  
‚úÖ Complete manifest for reproducibility  

---

## Output Files

After running, you get:

```
data/
  git_history_rich.jsonl              [All commits + metadata]
  git_history_rich.json               [Same, formatted]
  token_sequences_rich.json           [2048-token sequences]

embeddings/
  qdrant_points.json                  [768-dim vectors for RAG]

models/
  the-block-git-model-final/          [Trained GPT-2-medium]
    pytorch_model.bin
    config.json
    tokenizer.json

MANIFEST_DYNAMIC.json                 [Complete statistics]
```

---

## Understanding MANIFEST_DYNAMIC.json

```json
{
  "execution_timestamp": "2025-12-09T20:45:23.123456",
  "total_execution_time_seconds": 529.5,
  "repository_stats": {
    "unique_commits": 467,
    "total_commits_across_branches": 633,
    "branches": 5,
    "branch_names": ["main", "develop", "feature/energy-markets", "fix/edge-cases", "experimental/governance"],
    "unique_authors": 12,
    "commits_per_branch": {
      "main": 156,
      "develop": 342,
      "feature/energy-markets": 89,
      "fix/edge-cases": 34,
      "experimental/governance": 12
    },
    "time_span_days": 245.5,
    "commits_per_day": 1.90
  },
  "training_parameters": {
    "num_sequences": 78,
    "epochs": 6,
    "steps_per_epoch": 10,
    "total_steps": 60,
    "warmup_steps": 6,
    "batch_size": 8,
    "estimated_time_minutes": 1.5,
    "estimated_time_hours": 0.025
  },
  "phase_results": {
    "phase_0_analyze": {
      "status": "complete",
      "timestamp": "2025-12-09T20:45:23.456789"
    },
    "phase_1_scrape": {
      "status": "complete",
      "commits_processed": 467,
      "size_mb": 4.8,
      "duration_seconds": 134.2
    },
    "phase_2_tokenize": {
      "status": "complete",
      "num_sequences": 78,
      "total_tokens": 159744,
      "duration_seconds": 83.1
    },
    "phase_3_embeddings": {
      "status": "complete",
      "size_mb": 38.2,
      "embedding_dimension": 768,
      "duration_seconds": 225.3
    },
    "phase_4_training": {
      "status": "complete",
      "epochs": 6,
      "model_size_mb": 548,
      "duration_seconds": 67.5
    }
  }
}
```

---

## Training Statistics Tracking

The enhanced trainer tracks:

### Loss Metrics
- Training loss per batch
- Validation loss per epoch
- Perplexity (exponential of loss)
- Best validation loss
- Loss history for visualization

### Gradient Analysis
- Average gradient norm per epoch
- Maximum gradient norm
- Gradient clipping applied (1.0)

### Learning Rate Schedule
- Initial learning rate: 5e-5
- Warmup phase: 10% of total steps
- Linear decay after warmup
- LR tracked per step

### Hardware Monitoring
- GPU memory allocated (MB and %)
- GPU utilization (%)
- CPU usage (%)
- RAM usage (GB and %)
- Thermal temperature (¬∞C)
- Updated every N steps

### Timing
- Time per epoch
- Time per batch
- Total training time
- Estimated remaining time
- Extrapolated full run time

### Early Stopping
- Patience counter
- Best loss tracking
- Minimum delta for improvement
- Epochs without improvement

---

## When to Use Each Mode

### Dynamic Mode (Recommended)
```bash
python3 run_pipeline_dynamic.py --repo [path] --verbose
```

**Use when**:
- You want accurate statistics
- You're analyzing a new repository
- You want smart epoch determination
- You need comprehensive reporting

### Original Mode (Legacy)
```bash
python3 run_pipeline_optimized.py --repo [path] --verbose
```

**Use when**:
- You know your exact parameters
- You want specific epoch count
- You're running production repeats

---

## Troubleshooting

### "Could not detect branches"
```bash
# Make sure repo is valid Git
git -C /path/to/repo log --oneline | head
```

### "Very few commits detected"
```bash
# Make sure all branches are included
git -C /path/to/repo branch -a
git -C /path/to/repo log --all --oneline | wc -l
```

### Training stops early
```
# Early stopping is normal if validation loss plateaus
# Check MANIFEST_DYNAMIC.json for details
jq '.phase_results.phase_4_training' MANIFEST_DYNAMIC.json
```

### Different numbers than expected
```
# Dynamic mode counts correctly
# Previous assumptions might have been wrong
# New numbers are ACCURATE
```

---

## Example: Block Repository Analysis

```bash
$ python3 run_pipeline_dynamic.py --repo /Users/ianreitsma/projects/the-block --verbose

# System will:
# 1. Find all branches (main, develop, feature/*, fix/*, etc)
# 2. Count commits on each
# 3. Get true total (not 287, but actual)
# 4. Calculate optimal epochs
# 5. Show estimated time
# 6. Run full pipeline
# 7. Generate detailed statistics
```

---

## Summary

‚úÖ **Before**: 287 commits (assumed)  
‚úÖ **After**: [ACTUAL COUNT] commits (measured)  

‚úÖ **Before**: 5 epochs (hardcoded)  
‚úÖ **After**: [OPTIMAL] epochs (calculated)  

‚úÖ **Before**: No verification  
‚úÖ **After**: All branches scanned, all commits counted, comprehensive stats  

**Your system is now truly dynamic, accurate, and comprehensive.**
