# Quick Start Commands\n\n## Full Pipeline Execution\n\n```bash\n# Setup\ncd ~/.perplexity/git-scrape-scripting\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Step 1: Scrape\npython3 scrapers/git_scraper.py \\\n  --repo /Users/ianreitsma/projects/the-block \\\n  --output data/git_history.jsonl \\\n  --stats --verbose\n\n# Step 2: Tokenize\npython3 tokenizers/git_tokenizer.py \\\n  --input data/git_history.jsonl \\\n  --sequences data/token_sequences.json \\\n  --strategy semantic --stats\n\n# Step 3: Embeddings\npython3 embeddings/embedding_generator.py \\\n  --input data/git_history.jsonl \\\n  --qdrant-output embeddings/qdrant_points.json \\\n  --stats\n\n# Step 4: Train\npython3 training/model_trainer.py \\\n  --input data/token_sequences.json \\\n  --model-name gpt2 --epochs 3 --evaluate\n```\n\n## Individual Commands\n\n### Scraper\n```bash\npython3 scrapers/git_scraper.py --repo /path/to/repo --output data/git_history.jsonl --stats\n```\n\n### Tokenizer (Semantic - Recommended)\n```bash\npython3 tokenizers/git_tokenizer.py \\\n  --input data/git_history.jsonl \\\n  --strategy semantic \\\n  --sequences data/token_sequences.json \\\n  --stats\n```\n\n### Tokenizer (Other Strategies)\n```bash\n# Hierarchical (branch relationships)\npython3 tokenizers/git_tokenizer.py --input data/git_history.jsonl --strategy hierarchical\n\n# Diff-aware (code changes focus)\npython3 tokenizers/git_tokenizer.py --input data/git_history.jsonl --strategy diff_aware\n\n# Flat (simple sequential)\npython3 tokenizers/git_tokenizer.py --input data/git_history.jsonl --strategy flat\n```\n\n### Embedding Generator\n```bash\n# Fast (all-MiniLM-L6-v2)\npython3 embeddings/embedding_generator.py \\\n  --input data/git_history.jsonl \\\n  --model all-MiniLM-L6-v2 \\\n  --qdrant-output embeddings/qdrant_points.json\n\n# Better quality (all-mpnet-base-v2)\npython3 embeddings/embedding_generator.py \\\n  --input data/git_history.jsonl \\\n  --model all-mpnet-base-v2 \\\n  --qdrant-output embeddings/qdrant_points.json\n\n# Test similarity search\npython3 embeddings/embedding_generator.py \\\n  --input data/git_history.jsonl \\\n  --search-query \"energy market\"\n```\n\n### Model Training\n```bash\n# GPT-2 (fast, good)\npython3 training/model_trainer.py \\\n  --input data/token_sequences.json \\\n  --model-name gpt2 --epochs 3\n\n# Larger model (better, slower)\npython3 training/model_trainer.py \\\n  --input data/token_sequences.json \\\n  --model-name facebook/opt-350m --epochs 5 --batch-size 2\n\n# With evaluation\npython3 training/model_trainer.py \\\n  --input data/token_sequences.json \\\n  --epochs 3 --evaluate\n\n# CPU only (no GPU)\npython3 training/model_trainer.py \\\n  --input data/token_sequences.json \\\n  --no-gpu\n```\n\n## Troubleshooting Commands\n\n```bash\n# Check dependencies\npython3 -c \"import torch; print(f'PyTorch version: {torch.__version__}')\"\npython3 -c \"import transformers; print(f'Transformers: {transformers.__version__}')\"\n\n# Test GPU access\npython3 -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\npython3 -c \"import torch; print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\\"None\\\"}')\"\n\n# Verify Git access\ngit -C /Users/ianreitsma/projects/the-block log --oneline | head -5\n\n# Check file sizes\nls -lh data/\nls -lh embeddings/\nls -lh models/\n```\n\n## Performance Optimization\n\n### Mac M1 (Limited VRAM)\n```bash\n# Smaller batch size\npython3 training/model_trainer.py --batch-size 2 --epochs 5\n\n# Simpler tokenization\npython3 tokenizers/git_tokenizer.py --strategy flat\n\n# Faster embeddings\npython3 embeddings/embedding_generator.py --model all-MiniLM-L6-v2 --batch-size 64\n```\n\n### Linux Ryzen (GPU acceleration)\n```bash\n# With CUDA 11.8\npip install torch --index-url https://download.pytorch.org/whl/cu118\n\n# Larger batch size (GPU)\npython3 training/model_trainer.py --batch-size 8 --epochs 10\n\n# Larger model\npython3 training/model_trainer.py --model-name facebook/opt-1.3b --batch-size 4\n```\n\n## Data Inspection\n\n```bash\n# First 3 commits\nhead -3 data/git_history.jsonl | python3 -m json.tool\n\n# Commit count\nwc -l data/git_history.jsonl\n\n# Count tokens\npython3 -c \"import json; data = json.load(open('data/token_sequences.json')); print(f\\\"Sequences: {data['num_sequences']}, Tokens: {data['total_tokens']}\\\")\"\n\n# First embedding point\npython3 -c \"import json; data = json.load(open('embeddings/qdrant_points.json')); print(json.dumps(data[0], indent=2)[:500])\"\n\n# Model files\nls -lah models/the-block-git-model-final/\n```\n\n## Integration Points\n\n### For n8n (Docker)\n```bash\n# Start Qdrant\ndocker run -p 6333:6333 -v ~/qdrant_data:/qdrant/storage qdrant/qdrant\n\n# Start n8n\ndocker run -it --rm -p 5678:5678 n8nio/n8n:latest\n\n# Load embeddings into Qdrant (implement custom node)\n# POST http://localhost:6333/collections\n# with payload from embeddings/qdrant_points.json\n```\n\n### For Claude (Perplexity Pro)\n```python\n# Fetch relevant commits from embeddings\nimport json\n\nwith open('embeddings/commits.jsonl', 'r') as f:\n    commits = [json.loads(line) for line in f]\n\n# Send to Claude with context\nquery = \"Add energy market dispute RPC\"\nrelevant = [c for c in commits if 'energy' in c['metadata']['message_subject'].lower()][:3]\n\ncontext = \"\\n\".join([f\"{c['metadata']['commit_hash'][:8]}: {c['metadata']['message_subject']}\" for c in relevant])\nprint(f\"Context for Claude:\\n{context}\")\n```\n\n### For Local Models (Inference)\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = \"models/the-block-git-model-final\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n\n# Generate with context\ncontext = \"<COMMIT> a1b2c3d <AUTHOR> Ian <MESSAGE> Add energy market\"\ninputs = tokenizer(context, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=256, top_p=0.9, temperature=0.7)\nprint(tokenizer.decode(outputs[0]))\n```\n\n## File Locations\n\n```\nRepository Root:  ~/.perplexity/git-scrape-scripting/\n\nData:\n  - Scraped commits:  ./data/git_history.jsonl\n  - Tokenized commits: ./data/tokenized_commits.jsonl\n  - Token sequences:  ./data/token_sequences.json\n\nEmbeddings:\n  - With embeddings:  ./embeddings/commits.jsonl\n  - Qdrant format:    ./embeddings/qdrant_points.json\n\nModels:\n  - Trained model:    ./models/the-block-git-model-final/\n  - Weights:         ./models/the-block-git-model-final/pytorch_model.bin\n  - Config:          ./models/the-block-git-model-final/config.json\n  - Tokenizer:       ./models/the-block-git-model-final/tokenizer.json\n\nScripts:\n  - Scraper:         ./scrapers/git_scraper.py\n  - Tokenizer:       ./tokenizers/git_tokenizer.py\n  - Embeddings:      ./embeddings/embedding_generator.py\n  - Trainer:         ./training/model_trainer.py\n```\n\n## Expected Output Sizes\n\n```\nFor ~287 commits (The Block):\n\ngit_history.jsonl          ~2-3 MB\ntoken_sequences.json       ~0.5 MB\nqdrant_points.json         ~10-15 MB (embeddings)\nthe-block-git-model/       ~500-800 MB (model weights)\n\nTotal disk: ~1 GB\n```\n\n## Timeline Estimate\n\n```\nFirst run (all steps):\n  - Scraping:      ~1 min\n  - Tokenization:  ~1 min\n  - Embeddings:    ~3 min\n  - Training:      ~10-20 min (GPU faster)\n  - Total:         ~20-30 min\n\nSubsequent runs (incremental):\n  - Just embeddings: ~2 min\n  - Just training:   ~8-15 min\n```\n\n## Next: Deploy to Ryzen\n\n```bash\n# On Ryzen Linux with CUDA\ncd /home/user/projects/the-block/.perplexity/git-scrape-scripting\n\n# Setup\npython3 -m venv venv\nsource venv/bin/activate\npip install torch --index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\n\n# Run with GPU acceleration\npython3 training/model_trainer.py \\\n  --input data/token_sequences.json \\\n  --model-name gpt2 \\\n  --epochs 10 \\\n  --batch-size 8\n```\n