model:
  name: bigcode/starcoder2-3b
  tokenizer_name: bigcode/starcoder2-3b
  trust_remote_code: true
  use_4bit: true
  use_8bit: false
  use_bf16: true
  use_lora: true
  lora:
    r: 32
    lora_alpha: 64
    target_modules:
    - c_attn
    - c_proj
    - c_fc
    lora_dropout: 0.05
    bias: none
  max_position_embeddings: 2048
  max_new_tokens: 1024
  vocab_size_override: null
training:
  base_learning_rate: 5e-5
  warmup_ratio: 0.15
  warmup_steps_min: 20
  warmup_steps_max: 2000
  weight_decay: 0.01
  batch_size_reference: 1
  batch_size_large: 1
  batch_size_medium: 1
  batch_size_small: 1
  num_workers: 2
  num_workers_min: 1
  num_workers_max: 4
  pin_memory: true
  gradient_accumulation_steps: 16
  incremental_context_sequences: 3
  max_grad_norm: 1.0
  use_mixed_precision: true
  autocast_dtype: bfloat16
  use_gradient_checkpointing: true
  validation_split: 0.15
  patience: 8
  min_delta: 1.0e-05
  lr_plateau_patience: 5
  lr_reduction_factor: 0.5
  seed: 42
  curriculum_strategy: hierarchical_long_context
  long_context:
    phase1_max_length: 1024
    phase1_epochs: 5
    phase2_min_length: 1536
    phase2_max_length: 2048
    phase2_epochs: 8
    phase2_lr_multiplier: 0.5
    phase3_min_length: 3072
    phase3_max_length: 4096
    phase3_epochs: 8
    phase3_lr_multiplier: 0.1
    phase3_enabled: false
epoch_calculation:
  target_tokens: 50000000
  min_epochs: 15
  max_epochs: 30
hardware_monitoring:
  enabled: true
  interval_seconds: 10
  log_peaks: true
  memory_threshold_warn_pct: 85
  memory_threshold_stop_pct: 95
logging:
  level: INFO
  log_interval: 10
  save_interval: 500
  track_attention_patterns: true
  track_position_bias: true
  track_gradient_flow: true
model_saving:
  save_adapter_only: false
  save_best_model: true
  save_final_model: true
  save_ckpt_every_n_epochs: 0
