# TIER 5: 64K CONTEXT - EXTREME OPTIMIZATION
# 202x improvement over baseline (256 → 57,344 tokens!)
# Requires: FlashAttention-2, 8-bit optimizer, DeepSpeed ZeRO-2, KV cache compression
#
# Expected VRAM: ~7.4 GB / 7.5 GB (1% headroom - TIGHT!)
# Training speed: ~0.4-0.6 sequences/sec
# Quality: Very Good (LoRA rank 8 = 90% of full-rank performance)

model:
  name: "phi-2-ELITE-64k"
  pretrained_model: "microsoft/phi-2"  # 2.7B params
  add_special_tokens: true
  extra_special_tokens_max: 64
  trust_remote_code: true
  device_map: "auto"
  torch_dtype: "auto"
  mps_prefer_fp16: false

quantization:
  load_in_4bit: false
  load_in_8bit: true  # 8-bit quantization
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

  # TIER 5 LoRA: Extreme rank reduction for maximum context
  lora_enabled: true
  lora_rank: 8  # Extreme reduction → frees ~2.1 GB in optimizer states
  lora_alpha: 16  # 2x rank
  lora_dropout: 0.03
  lora_target_modules: ["q_proj", "v_proj", "up_proj", "down_proj"]
  lora_bias: "none"

  train_path: "training_data_ELITE/training_data_train.jsonl"
  val_path: "training_data_ELITE/training_data_val.jsonl"
  test_path: "training_data_ELITE/training_data_test.jsonl"

  # TIER 5: 64K CONTEXT WINDOW! (Actually 56K for safety)
  # With FlashAttention-2 + DeepSpeed CPU offloading + aggressive compression
  # This is 224x improvement over original 256 tokens
  context_window: 57344  # ← 56K tokens! (~14,000 lines of Rust code!!!)
  target_window: 7168    # ← 7K tokens! (~1,800 lines generated!!!)

optimization:
  # TIER 5: Optimized for 64K sequences
  batch_size: 1
  gradient_accumulation_steps: 8  # Effective batch = 8

  learning_rate: 6.0e-4  # Slightly lower for extreme context
  lr_scheduler: "cosine"
  warmup_steps: 1500
  warmup_ratio: 0.05

  optimizer: "adamw"  # Will use AdamW8bit from bitsandbytes
  weight_decay: 0.05
  max_grad_norm: 1.0

  mixed_precision: "fp16"
  use_mixed_precision: true

  # CRITICAL: Gradient checkpointing
  gradient_checkpointing: true

training:
  num_epochs: 20
  max_steps: 200000
  save_steps: 17780  # ~1 epoch
  save_total_limit: 10  # Keep fewer checkpoints (disk space)
  eval_steps: 1778
  logging_steps: 200
  seed: 42

  lr_plateau_patience: 20
  patience: 20

inference:
  max_length: 16384  # Can generate up to 16K tokens
  do_sample: true
  top_p: 0.95
  temperature: 0.7
  num_beams: 1

device_backend:
  force_device: null
  attention_backend: "auto"

output:
  output_dir: "models/the-block-ELITE-TIER5-64kctx"
  overwrite_output_dir: true
  save_model: true
  save_optimizer: false
