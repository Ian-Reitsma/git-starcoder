# Universal Training Config: Works on macOS (Metal) + Linux (CUDA)
#
# This config auto-detects device and adapts settings accordingly.
# No manual device selection needed—just run the same command on any platform.

model:
  name: "codebert-optimized"
  pretrained_model: "microsoft/codebert-base"  # 125M CODE-SPECIALIZED, perfect for 7.6GB
  # CodeBERT is trained specifically on code AND markdown documentation
  # 8.3B training tokens from 6.4M functions + documentation
  # Better than generic models for code understanding
  add_special_tokens: true
  extra_special_tokens_max: 64
  trust_remote_code: true
  device_map: "auto"

  torch_dtype: "auto"
  mps_prefer_fp16: false

quantization:
  load_in_4bit: false
  load_in_8bit: true  # 8-bit is MORE efficient than 4-bit for training on this model
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

  # AGGRESSIVE LoRA: For CodeBERT with large dataset (11,551 sequences)
  lora_enabled: true
  lora_rank: 96  # INCREASED: larger rank for complex Rust/code patterns in the-block
  lora_alpha: 192  # 2x rank for maximum expressiveness
  lora_dropout: 0.03  # Lower dropout (more data to prevent overfitting)
  lora_target_modules: ["query", "value", "key", "dense"]  # Include dense layer for FFN
  lora_bias: "none"


  train_path: "data/scrape-dec23/training_data_train.json"
  val_path: "data/scrape-dec23/data_enhanced/training_data_val.json"
  test_path: "data/scrape-dec23/training_data_test.json"

  context_window: 512  # INCREASED: 4x more context = better code understanding
  target_window: 128   # INCREASED: 4x larger target = better prediction quality

optimization:
  # Memory-efficient approach for 11,551 sequences: effective batch 16 on 7.6GB
  batch_size: 1  # Keep at 1 (LoRA + 8-bit + gradient checkpointing)
  gradient_accumulation_steps: 16  # INCREASED from 8: simulates batch size 16 for better convergence
                                    # With 11K sequences, larger effective batch helps stability

  # AGGRESSIVE learning strategy: aim for 10x better results
  learning_rate: 1.2e-3  # INCREASED: higher LR works well with LoRA rank 96 + large dataset
  lr_scheduler: "cosine"  # Cosine annealing for smooth convergence
  warmup_steps: 1000  # INCREASED: important with larger dataset for stable warmup
  warmup_ratio: 0.05

  optimizer: "adamw"
  weight_decay: 0.08  # INCREASED: stronger L2 regularization for 11K sequences
  max_grad_norm: 0.3  # TIGHTER: stricter gradient clipping for stable LoRA with larger rank

  mixed_precision: "fp16"  # Keep fp16 for speed
  use_mixed_precision: true

  # Critical for 7.6GB: save memory without hurting quality
  gradient_checkpointing: true

training:
  num_epochs: 200  # MASSIVELY INCREASED: allow comprehensive training on 11.5K sequences
  max_steps: 200000  # Give ample room to converge (11,551 train * 16 accumulation / 16 acc = 726 steps/epoch)
  save_steps: 728  # Save after each epoch on 11.5K sequences
  save_total_limit: 10  # Keep best 10 checkpoints
  eval_steps: 91  # Evaluate frequently (every ~91 steps ≈ every 1.3K samples)
  logging_steps: 45  # Log very frequently
  seed: 42

  # EARLY STOPPING PATIENCE - critical for converging on full dataset
  lr_plateau_patience: 15  # More patience with larger dataset (stop if no improvement for 15 epochs)
  patience: 15

inference:
  # Long-form codegen relies on iterative generation + compile-gated patching
  max_length: 4096
  do_sample: true
  top_p: 0.95
  temperature: 0.7
  num_beams: 1

device_backend:
  # Auto-detect platform; can override with explicit device
  force_device: null  # null = auto-detect ("cuda", "mps", or "cpu")
  
  # Attention backend will be auto-selected:
  # - macOS (Metal): "metal" (with FlashAttention if available)
  # - Linux (CUDA, Turing): "sdpa"
  # - Linux (CUDA, Ampere+): "xformers"
  attention_backend: "auto"

output:
  output_dir: "models/the-block-enhanced-metal-cuda"
  overwrite_output_dir: false
  save_model: true
  save_optimizer: false
