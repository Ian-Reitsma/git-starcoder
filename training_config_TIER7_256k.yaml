# TIER 7: 256K CONTEXT - CRAZY MODE!!!
# 922x improvement over baseline (256 → 262,144 tokens!)
# Requires: EVERYTHING + larger chunks + extreme compression
#
# Expected VRAM: ~7.97 GB / 8.0 GB (VERY TIGHT!)
# Training speed: ~0.2-0.3 sequences/sec
# Quality: Good (LoRA rank 6 = 85% of full-rank)
#
# THIS IS RESEARCH FRONTIER - Almost entire codebase in context!

model:
  name: "phi-2-ELITE-256k"
  pretrained_model: "microsoft/phi-2"
  add_special_tokens: true
  extra_special_tokens_max: 64
  trust_remote_code: true
  device_map: "auto"
  torch_dtype: "auto"
  mps_prefer_fp16: false

quantization:
  load_in_4bit: false
  load_in_8bit: true
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

  # TIER 7 LoRA: EXTREME reduction
  lora_enabled: true
  lora_rank: 6  # EXTREME (but proven to work in research)
  lora_alpha: 12
  lora_dropout: 0.03
  lora_target_modules: ["q_proj", "v_proj", "up_proj", "down_proj"]
  lora_bias: "none"

  train_path: "training_data_ELITE/training_data_train.jsonl"
  val_path: "training_data_ELITE/training_data_val.jsonl"
  test_path: "training_data_ELITE/training_data_test.jsonl"

  # TIER 7: 256K CONTEXT WINDOW!!!!
  # Almost your ENTIRE codebase in one context!
  # This is 1024x improvement over original 256 tokens
  context_window: 262144  # ← 256K tokens! (~65,000 lines!!!)
  target_window: 32768    # ← 32K tokens! (~8,000 lines generated!!!)

  # Ring Attention configuration (larger chunks)
  ring_attention_enabled: true
  ring_chunk_size: 12288  # 12K chunks for 256K context
  ring_use_flash: true
  ring_compression: "int4"  # Aggressive KV cache compression

optimization:
  batch_size: 1
  gradient_accumulation_steps: 8

  learning_rate: 4.0e-4  # Even lower for extreme stability
  lr_scheduler: "cosine"
  warmup_steps: 3000
  warmup_ratio: 0.15  # Long warmup

  optimizer: "adamw"
  weight_decay: 0.05
  max_grad_norm: 0.5  # Tighter clipping for stability

  mixed_precision: "fp16"
  use_mixed_precision: true

  gradient_checkpointing: true

training:
  num_epochs: 10  # Even fewer epochs (very expensive)
  max_steps: 200000
  save_steps: 17780
  save_total_limit: 3  # Minimal checkpoints
  eval_steps: 1778
  logging_steps: 50
  seed: 42

  lr_plateau_patience: 10
  patience: 10

inference:
  max_length: 65536  # Can generate up to 64K tokens!!!
  do_sample: true
  top_p: 0.95
  temperature: 0.7
  num_beams: 1

device_backend:
  force_device: null
  attention_backend: "ring"

output:
  output_dir: "models/the-block-ELITE-TIER7-256kctx"
  overwrite_output_dir: true
  save_model: true
  save_optimizer: false
